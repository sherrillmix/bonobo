## Prepare reads and run QIIME

```{r}
#stop on errors
knitr::opts_chunk$set(error=FALSE,tidy=TRUE)
```

### Load libraries
```{r}
# installed from https://github.com/sherrillmix/dnar 
library(dnar)
packageVersion('dnar')
library(parallel)
packageVersion('parallel')
source('../functions.R')
```

### Software versions
```{r}
system('echo "source activate qiime1;print_qiime_config.py -t"|bash 2>&1',intern=TRUE)
system('bbmerge.sh --version 2>&1',intern=TRUE)
```

### Merge reads
```{r,cache=TRUE}
  if(!dir.exists('data/joined'))dir.create('data/joined')
  fastq1s<-list.files('data','_R1_.*\\.fastq.gz',full.name=TRUE)
  mclapply(fastq1s,function(ii){
    out<-sprintf('data/joined/%s.fastq',sub('_R1_.*$','',basename(ii)))
    cmd<-sprintf('bbmerge.sh in1=%s in2=%s out=%s t=10 2>%s',ii,sub('_R1_','_R2_',ii),out,sub('fastq$','out',out))
    exit<-system(cmd)
    if(exit!=0)stop("Problem running pairing in",ii)
    system(sprintf('gzip %s',out))
    return(cmd)
  },mc.cores=10)
```

### Read in all sequences
```{r readFastqs,cache=TRUE}
fastqs<-list.files('data/joined','.fastq.gz',full.name=TRUE)
#just get sequences to reduce memory
allSeq<-mclapply(fastqs,function(xx)read.fastq(xx)$seq,mc.cores=12)
message('Read ',length(unlist(allSeq)),' sequences in ',length(allSeq),' fastqs')
```

### Run qiime
```{r runQiime,cache=TRUE,dependson='readFastqs'}
if(!dir.exists('work/'))dir.create('work')
out<-runQiime(unlist(allSeq),storeDir='work/qiime')
outDf<-data.frame(
  'file'=rep(sub('_16s.fastq.gz','',basename(fastqs)),sapply(allSeq,length)),
  'otu'=out[['otus']],
  stringsAsFactors=FALSE
)
withAs(outFile=gzfile('work/qiimeOtuIds.csv.gz'),write.csv(outDf,outFile,row.names=FALSE))
write.fa(names(out[['seqs']]),out[['seqs']],'work/qiimeOtus.fa.gz')
outTaxa<-data.frame(
  'name'=names(out[['taxa']]),
  'taxa'=out[['taxa']],
  stringsAsFactors=FALSE
)
write.csv(outTaxa,'work/qiimeOtus.taxa',row.names=FALSE)
```
